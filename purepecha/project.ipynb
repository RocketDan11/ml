{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69169/87782857.py:240: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Information:\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "Current CUDA device: 0\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti\n",
      "CUDA device count: 1\n",
      "CUDA device properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080 Ti', major=8, minor=6, total_memory=12004MB, multi_processor_count=80, uuid=2c3e1dc0-0627-ada5-0f67-7530e2fdf2af, L2_cache_size=6MB)\n",
      "\n",
      "Loaded 213 translation pairs\n",
      "Source vocabulary size: 48\n",
      "Target vocabulary size: 47\n",
      "Training samples: 170\n",
      "Validation samples: 43\n",
      "\n",
      "Model Summary:\n",
      "Total parameters: 6,361,647\n",
      "Trainable parameters: 6,361,647\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   0%|          | 0/6 [00:00<?, ?it/s]/tmp/ipykernel_69169/87782857.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/10 [Train]: 100%|██████████| 6/6 [00:00<00:00, 31.65it/s, loss=3.0880, acc=0.2724, gpu_mem=77.0MB]\n",
      "Epoch 1/10 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_69169/87782857.py:300: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/10 [Val]: 100%|██████████| 2/2 [00:00<00:00, 17.23it/s, loss=2.3589, acc=0.3800, gpu_mem=77.1MB]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tgt_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 410\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal BLEU Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_bleu_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[16], line 391\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainable parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainable_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n\u001b[1;32m    394\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[16], line 325\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    322\u001b[0m avg_val_acc \u001b[38;5;241m=\u001b[39m val_acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m calculate_bleu_score(all_predictions, all_targets, \u001b[43mtgt_vocab\u001b[49m)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[1;32m    328\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(avg_val_loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tgt_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch.cuda as cuda\n",
    "import torch.backends.cudnn as cudnn\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Hyperparameters\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "MLP_DIM = 512\n",
    "DROPOUT = 0.1\n",
    "MAX_LENGTH = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Custom Multi-Head Self-Attention module\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(\n",
    "            in_features=embed_dim,\n",
    "            hidden_features=mlp_dim,\n",
    "            out_features=embed_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class PurepechaTranslator(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, EMBED_DIM)\n",
    "        self.output_embedding = nn.Embedding(output_vocab_size, EMBED_DIM)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(EMBED_DIM, NUM_HEADS, MLP_DIM, DROPOUT)\n",
    "            for _ in range(NUM_LAYERS)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(EMBED_DIM, NUM_HEADS, MLP_DIM, DROPOUT)\n",
    "            for _ in range(NUM_LAYERS)\n",
    "        ])\n",
    "        \n",
    "        self.final_layer = nn.Linear(EMBED_DIM, output_vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # src: [batch_size, seq_len]\n",
    "        # tgt: [batch_size, seq_len]\n",
    "        \n",
    "        src_emb = self.input_embedding(src)\n",
    "        tgt_emb = self.output_embedding(tgt)\n",
    "        \n",
    "        # Encoder\n",
    "        enc_out = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_out = layer(enc_out)\n",
    "            \n",
    "        # Decoder\n",
    "        dec_out = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_out = layer(dec_out)\n",
    "            \n",
    "        # Final projection\n",
    "        output = self.final_layer(dec_out)\n",
    "        return output\n",
    "\n",
    "class PurepechaDataset(Dataset):\n",
    "    def __init__(self, src_texts, tgt_texts, src_vocab, tgt_vocab):\n",
    "        self.src_texts = src_texts\n",
    "        self.tgt_texts = tgt_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_length = MAX_LENGTH\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_texts[idx]\n",
    "        tgt_text = self.tgt_texts[idx]\n",
    "        \n",
    "        # Convert text to indices\n",
    "        src_indices = [self.src_vocab.get(char, self.src_vocab['<unk>']) for char in src_text]\n",
    "        tgt_indices = [self.tgt_vocab.get(char, self.tgt_vocab['<unk>']) for char in tgt_text]\n",
    "        \n",
    "        # Pad sequences\n",
    "        src_indices = src_indices[:self.max_length] + [self.src_vocab['<pad>']] * (self.max_length - len(src_indices))\n",
    "        tgt_indices = tgt_indices[:self.max_length] + [self.tgt_vocab['<pad>']] * (self.max_length - len(tgt_indices))\n",
    "        \n",
    "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
    "\n",
    "def create_vocab(texts):\n",
    "    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
    "    for text in texts:\n",
    "        for char in text:\n",
    "            if char not in vocab:\n",
    "                vocab[char] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, names=['english', 'purepecha'])\n",
    "    return df['english'].tolist(), df['purepecha'].tolist()\n",
    "\n",
    "def print_torch_info():\n",
    "    print(\"\\nPyTorch Information:\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {cuda.is_available()}\")\n",
    "    if cuda.is_available():\n",
    "        print(f\"Current CUDA device: {cuda.current_device()}\")\n",
    "        print(f\"Device name: {cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA device count: {cuda.device_count()}\")\n",
    "        print(f\"CUDA device properties: {cuda.get_device_properties(0)}\")\n",
    "    print()\n",
    "\n",
    "def calculate_accuracy(predictions, targets, pad_idx=0):\n",
    "    # Calculate accuracy ignoring padding tokens\n",
    "    mask = targets != pad_idx\n",
    "    correct = (predictions == targets) * mask\n",
    "    return correct.sum().item() / mask.sum().item()\n",
    "\n",
    "def calculate_bleu_score(predictions, references, tgt_vocab):\n",
    "    # Convert indices to tokens\n",
    "    idx_to_token = {idx: token for token, idx in tgt_vocab.items()}\n",
    "    \n",
    "    # Convert predictions and references to text\n",
    "    pred_texts = []\n",
    "    ref_texts = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Convert indices to tokens, ignoring padding\n",
    "        pred_tokens = [idx_to_token[idx] for idx in pred if idx not in [tgt_vocab['<pad>'], tgt_vocab['<sos>'], tgt_vocab['<eos>']]]\n",
    "        ref_tokens = [idx_to_token[idx] for idx in ref if idx not in [tgt_vocab['<pad>'], tgt_vocab['<sos>'], tgt_vocab['<eos>']]]\n",
    "        \n",
    "        pred_texts.append(pred_tokens)\n",
    "        ref_texts.append([ref_tokens])  # BLEU expects references as list of lists\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for pred, ref in zip(pred_texts, ref_texts):\n",
    "        try:\n",
    "            score = sentence_bleu(ref, pred, smoothing_function=smoothie)\n",
    "            bleu_scores.append(score)\n",
    "        except:\n",
    "            # Skip if there's an error (e.g., empty sequences)\n",
    "            continue\n",
    "    \n",
    "    return np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "\n",
    "def setup_gpu():\n",
    "    if not cuda.is_available():\n",
    "        print(\"Warning: CUDA is not available. Training will be performed on CPU.\")\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "    # Set CUDA device\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # Enable cuDNN benchmarking for faster training\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    # Set deterministic mode for reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(42)\n",
    "    if cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return device\n",
    "\n",
    "def train_model(model, train_loader, val_loader, tgt_vocab):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    \n",
    "    # Setup GPU\n",
    "    device = setup_gpu()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable gradient scaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_bleu_score = 0.0\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Train]')\n",
    "        for src, tgt in train_pbar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Use automatic mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(src, tgt[:, :-1])\n",
    "                loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            \n",
    "            # Scale gradients and update weights\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = output.argmax(dim=-1)\n",
    "            acc = calculate_accuracy(predictions, tgt[:, 1:])\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{train_loss/train_batches:.4f}',\n",
    "                'acc': f'{train_acc/train_batches:.4f}',\n",
    "                'gpu_mem': f'{cuda.memory_allocated()/1024**2:.1f}MB'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_acc = train_acc / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Val]')\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_pbar:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(src, tgt[:, :-1])\n",
    "                    loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "                \n",
    "                predictions = output.argmax(dim=-1)\n",
    "                acc = calculate_accuracy(predictions, tgt[:, 1:])\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_acc += acc\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Store predictions and targets for BLEU score\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_targets.extend(tgt[:, 1:].cpu().numpy())\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{val_loss/val_batches:.4f}',\n",
    "                    'acc': f'{val_acc/val_batches:.4f}',\n",
    "                    'gpu_mem': f'{cuda.memory_allocated()/1024**2:.1f}MB'\n",
    "                })\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_acc = val_acc / len(val_loader)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu_score = calculate_bleu_score(all_predictions, all_targets, tgt_vocab)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS} Summary:')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f} | Training Accuracy: {avg_train_acc:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {avg_val_acc:.4f}')\n",
    "        print(f'BLEU Score: {bleu_score:.4f}')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        print(f'GPU Memory Usage: {cuda.memory_allocated()/1024**2:.1f}MB')\n",
    "        \n",
    "        # Save best model based on BLEU score\n",
    "        if bleu_score > best_bleu_score:\n",
    "            best_bleu_score = bleu_score\n",
    "            best_model_state = model.state_dict()\n",
    "            print('New best model saved!')\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    print_torch_info()\n",
    "    \n",
    "    # Load data\n",
    "    src_texts, tgt_texts = load_data('assets/purepecha_data.tsv')\n",
    "    print(f\"Loaded {len(src_texts)} translation pairs\")\n",
    "    \n",
    "    # Create vocabularies\n",
    "    src_vocab = create_vocab(src_texts)\n",
    "    tgt_vocab = create_vocab(tgt_texts)\n",
    "    print(f\"Source vocabulary size: {len(src_vocab)}\")\n",
    "    print(f\"Target vocabulary size: {len(tgt_vocab)}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_src, val_src, train_tgt, val_tgt = train_test_split(\n",
    "        src_texts, tgt_texts, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training samples: {len(train_src)}\")\n",
    "    print(f\"Validation samples: {len(val_src)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PurepechaDataset(train_src, train_tgt, src_vocab, tgt_vocab)\n",
    "    val_dataset = PurepechaDataset(val_src, val_tgt, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # Create dataloaders with num_workers for faster data loading\n",
    "    num_workers = min(4, cuda.device_count() * 4) if cuda.is_available() else 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PurepechaTranslator(\n",
    "        input_vocab_size=len(src_vocab),\n",
    "        output_vocab_size=len(tgt_vocab)\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nModel Summary:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(model, train_loader, val_loader, tgt_vocab)\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            predictions = output.argmax(dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(tgt[:, 1:].cpu().numpy())\n",
    "    \n",
    "    final_bleu_score = calculate_bleu_score(all_predictions, all_targets, tgt_vocab)\n",
    "    print(f\"\\nFinal BLEU Score: {final_bleu_score:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
