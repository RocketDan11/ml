{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Information:\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 341\u001b[0m\n\u001b[1;32m    338\u001b[0m     model \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 341\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[13], line 296\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mprint_torch_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     src_texts, tgt_texts \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massets/purepecha_data.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 148\u001b[0m, in \u001b[0;36mprint_torch_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda\u001b[38;5;241m.\u001b[39mis_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent CUDA device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda\u001b[38;5;241m.\u001b[39mget_device_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch.cuda as cuda\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# Custom Multi-Head Self-Attention module\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(\n",
    "            in_features=embed_dim,\n",
    "            hidden_features=mlp_dim,\n",
    "            out_features=embed_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class PurepechaTranslator(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embed_dim=256, num_heads=8, num_layers=6, mlp_dim=512):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, embed_dim)\n",
    "        self.output_embedding = nn.Embedding(output_vocab_size, embed_dim)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_layer = nn.Linear(embed_dim, output_vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # src: [batch_size, seq_len]\n",
    "        # tgt: [batch_size, seq_len]\n",
    "        \n",
    "        src_emb = self.input_embedding(src)\n",
    "        tgt_emb = self.output_embedding(tgt)\n",
    "        \n",
    "        # Encoder\n",
    "        enc_out = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_out = layer(enc_out)\n",
    "            \n",
    "        # Decoder\n",
    "        dec_out = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_out = layer(dec_out)\n",
    "            \n",
    "        # Final projection\n",
    "        output = self.final_layer(dec_out)\n",
    "        return output\n",
    "\n",
    "class PurepechaDataset(Dataset):\n",
    "    def __init__(self, src_texts, tgt_texts, src_vocab, tgt_vocab, max_length=50):\n",
    "        self.src_texts = src_texts\n",
    "        self.tgt_texts = tgt_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_texts[idx]\n",
    "        tgt_text = self.tgt_texts[idx]\n",
    "        \n",
    "        # Convert text to indices\n",
    "        src_indices = [self.src_vocab.get(char, self.src_vocab['<unk>']) for char in src_text]\n",
    "        tgt_indices = [self.tgt_vocab.get(char, self.tgt_vocab['<unk>']) for char in tgt_text]\n",
    "        \n",
    "        # Pad sequences\n",
    "        src_indices = src_indices[:self.max_length] + [self.src_vocab['<pad>']] * (self.max_length - len(src_indices))\n",
    "        tgt_indices = tgt_indices[:self.max_length] + [self.tgt_vocab['<pad>']] * (self.max_length - len(tgt_indices))\n",
    "        \n",
    "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
    "\n",
    "def create_vocab(texts):\n",
    "    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
    "    for text in texts:\n",
    "        for char in text:\n",
    "            if char not in vocab:\n",
    "                vocab[char] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, names=['english', 'purepecha'])\n",
    "    return df['english'].tolist(), df['purepecha'].tolist()\n",
    "\n",
    "def print_torch_info():\n",
    "    print(\"\\nPyTorch Information:\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {cuda.is_available()}\")\n",
    "    if cuda.is_available():\n",
    "        print(f\"Current CUDA device: {cuda.current_device()}\")\n",
    "        print(f\"Device name: {cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA device count: {cuda.device_count()}\")\n",
    "        print(f\"CUDA device properties: {cuda.get_device_properties(0)}\")\n",
    "    print()\n",
    "\n",
    "def calculate_accuracy(predictions, targets, pad_idx=0):\n",
    "    # Calculate accuracy ignoring padding tokens\n",
    "    mask = targets != pad_idx\n",
    "    correct = (predictions == targets) * mask\n",
    "    return correct.sum().item() / mask.sum().item()\n",
    "\n",
    "def setup_gpu():\n",
    "    if not cuda.is_available():\n",
    "        print(\"Warning: CUDA is not available. Training will be performed on CPU.\")\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "    # Set CUDA device\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # Enable cuDNN benchmarking for faster training\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    # Set deterministic mode for reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(42)\n",
    "    if cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return device\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    \n",
    "    # Setup GPU\n",
    "    device = setup_gpu()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable gradient scaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for src, tgt in train_pbar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Use automatic mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(src, tgt[:, :-1])\n",
    "                loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            \n",
    "            # Scale gradients and update weights\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = output.argmax(dim=-1)\n",
    "            acc = calculate_accuracy(predictions, tgt[:, 1:])\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{train_loss/train_batches:.4f}',\n",
    "                'acc': f'{train_acc/train_batches:.4f}',\n",
    "                'gpu_mem': f'{cuda.memory_allocated()/1024**2:.1f}MB'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_acc = train_acc / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_pbar:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(src, tgt[:, :-1])\n",
    "                    loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "                \n",
    "                predictions = output.argmax(dim=-1)\n",
    "                acc = calculate_accuracy(predictions, tgt[:, 1:])\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_acc += acc\n",
    "                val_batches += 1\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{val_loss/val_batches:.4f}',\n",
    "                    'acc': f'{val_acc/val_batches:.4f}',\n",
    "                    'gpu_mem': f'{cuda.memory_allocated()/1024**2:.1f}MB'\n",
    "                })\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_acc = val_acc / len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} Summary:')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f} | Training Accuracy: {avg_train_acc:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {avg_val_acc:.4f}')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        print(f'GPU Memory Usage: {cuda.memory_allocated()/1024**2:.1f}MB')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            print('New best model saved!')\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    print_torch_info()\n",
    "    \n",
    "    # Load data\n",
    "    src_texts, tgt_texts = load_data('assets/purepecha_data.tsv')\n",
    "    print(f\"Loaded {len(src_texts)} translation pairs\")\n",
    "    \n",
    "    # Create vocabularies\n",
    "    src_vocab = create_vocab(src_texts)\n",
    "    tgt_vocab = create_vocab(tgt_texts)\n",
    "    print(f\"Source vocabulary size: {len(src_vocab)}\")\n",
    "    print(f\"Target vocabulary size: {len(tgt_vocab)}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_src, val_src, train_tgt, val_tgt = train_test_split(\n",
    "        src_texts, tgt_texts, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training samples: {len(train_src)}\")\n",
    "    print(f\"Validation samples: {len(val_src)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PurepechaDataset(train_src, train_tgt, src_vocab, tgt_vocab)\n",
    "    val_dataset = PurepechaDataset(val_src, val_tgt, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # Create dataloaders with num_workers for faster data loading\n",
    "    num_workers = min(4, cuda.device_count() * 4) if cuda.is_available() else 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PurepechaTranslator(\n",
    "        input_vocab_size=len(src_vocab),\n",
    "        output_vocab_size=len(tgt_vocab)\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nModel Summary:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(model, train_loader, val_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
